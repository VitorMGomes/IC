from langchain_ollama import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains import RetrievalQA
from vector import retriever
import streamlit as st

# Modelo de linguagem
model = OllamaLLM(model="llama3.2", streaming=False)

# Prompt base
template = """
Voc√™ √© respons√°vel pela folha de pagamento de uma empresa.

Aqui est√£o os dados relevantes:
{context}

Baseando-se nesses dados, responda √† seguinte pergunta: {question}
"""

prompt = ChatPromptTemplate.from_template(template)

# Cria o chain com recupera√ß√£o vetorial
qa_chain = RetrievalQA.from_chain_type(
    llm=model,
    retriever=retriever,
    chain_type_kwargs={"prompt": prompt}
)

# T√≠tulo do app
st.title("üßæ Slip Pay Agent")

# Inicializa hist√≥rico de mensagens
if "messages" not in st.session_state:
    st.session_state.messages = []

# Exibe hist√≥rico
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Entrada do usu√°rio
user_input = st.chat_input("Digite sua pergunta sobre a folha de pagamento")

if user_input:
    # Mostra mensagem do usu√°rio
    st.chat_message("user").markdown(user_input)
    st.session_state.messages.append({"role": "user", "content": user_input})

    # Consulta o modelo via LangChain
    with st.spinner("Consultando IA..."):
        result = qa_chain.invoke({"query": user_input})
        response = result["result"]

    # Mostra resposta do agente
    with st.chat_message("assistant"):
        st.markdown(response)

    # Salva no hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": response})
